{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fbacc20",
   "metadata": {},
   "source": [
    "## What are Hyperparameters ? and How to tune the Hyperparameters in a Deep Neural Network?\n",
    "\n",
    "### What are hyperparameters?\n",
    "\n",
    "Hyperparameters are the variables which determines the network structure(Eg: Number of Hidden Units) and the variables which determine how the network is trained(Eg: Learning Rate).\n",
    "\n",
    "Hyperparameters are set before training(before optimizing the weights and bias).\n",
    "\n",
    "### Hyperparameters related to Network structure\n",
    "\n",
    "<b> Number of Hidden Layers and units\n",
    "\n",
    "Hidden layers are the layers between input layer and output layer.\n",
    "\n",
    "“Very simple. Just keep adding layers until the test error does not improve anymore.”\n",
    "\n",
    "Many hidden units within a layer with regularization techniques can increase accuracy. Smaller number of units may cause underfitting.\n",
    "\n",
    "    \n",
    "<b> Dropout\n",
    "\n",
    "Dropout is regularization technique to avoid overfitting (increase the validation accuracy) thus increasing the generalizing power.\n",
    "\n",
    "- Generally, use a small dropout value of 20%-50% of neurons with 20% providing a good starting point. A probability too low has minimal effect and a value too high results in under-learning by the network.\n",
    "    \n",
    "    \n",
    "- Use a larger network. You are likely to get better performance when dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.\n",
    "    \n",
    "    \n",
    "### Network Weight Initialization\n",
    "    \n",
    "Ideally, it may be better to use different weight initialization schemes according to the activation function used on each layer.\n",
    "\n",
    "Mostly uniform distribution is used.\n",
    "\n",
    "### Activation function\n",
    "\n",
    "\n",
    "Activation functions are used to introduce nonlinearity to models, which allows deep learning models to learn nonlinear prediction boundaries.\n",
    "\n",
    "Generally, the rectifier activation function is the most popular.\n",
    "\n",
    "Sigmoid is used in the output layer while making binary predictions. Softmax is used in the output layer while making multi-class predictions.\n",
    "\n",
    "### Hyperparameters related to Training Algorithm\n",
    "    \n",
    "<b> Learning Rate\n",
    "\n",
    "\n",
    "The learning rate defines how quickly a network updates its parameters.\n",
    "\n",
    "Low learning rate slows down the learning process but converges smoothly. Larger learning rate speeds up the learning but may not converge.\n",
    "\n",
    "Usually a decaying Learning rate is preferred.\n",
    "\n",
    "<b> Momentum\n",
    "    \n",
    "Momentum helps to know the direction of the next step with the knowledge of the previous steps. It helps to prevent oscillations. A typical choice of momentum is between 0.5 to 0.9.\n",
    "\n",
    "\n",
    "<b> Number of epochs\n",
    "    \n",
    "Number of epochs is the number of times the whole training data is shown to the network while training.\n",
    "\n",
    "Increase the number of epochs until the validation accuracy starts decreasing even when training accuracy is increasing(overfitting).\n",
    "\n",
    "    \n",
    "<b> Batch size\n",
    "    \n",
    "Mini batch size is the number of sub samples given to the network after which parameter update happens.\n",
    "\n",
    "A good default for batch size might be 32. Also try 32, 64, 128, 256, and so on.\n",
    "\n",
    "### Methods used to find out Hyperparameters\n",
    "    \n",
    "- Manual Search\n",
    "- Grid Search (http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)\n",
    "- Random Search\n",
    "- Bayesian Optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
