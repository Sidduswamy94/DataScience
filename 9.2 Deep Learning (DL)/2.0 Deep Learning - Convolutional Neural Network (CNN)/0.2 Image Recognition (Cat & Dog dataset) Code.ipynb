{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bafb364",
   "metadata": {},
   "source": [
    "# Image Recognition (Cat & Dog dataset)\n",
    "<b> In our dataset, we have all the images of cats and dogs in training as well as in the test set folders. We are going to train our CNN model on 4048 images of cats as well as 4000 images of dogs, each respectively that are present in the training set followed by evaluating our model with the new 1000 images of cats and 1000 images of dogs, each respectively in the test set on which our model was not trained. So, we are actually going to build and train a Convolutional Neural network to recognize if there is a dog or cat in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39954c75",
   "metadata": {},
   "source": [
    "### importing all the necessary libraries.\n",
    "    \n",
    "Here we are importing the TensorFlow, keras library and actually the preprocessing module by Keras library. And then, we will import the image sub-module of the preprocessing module of the Keras library, which will allow us to do image pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b5af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc3639",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "<b> Preprocessing the Training set\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12737adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443ae2e",
   "metadata": {},
   "source": [
    "In the above, We have created an object of 'train_datagen' of the ImageDataGenerator class that represents the tool that will apply all the transformations on the images of the training set, such that the **rescale** argument will apply feature scaling to each and every single one the pixel by dividing their value 255 as each pixel take a value between 0 and 255, which is really necessary for neural networks and the rest are the transformations that will perform image augmentation on the training set images so as to prevent the overfitting. **shear_range:** Shear Intensity (Shear angle in counter-clockwise direction in degrees). **zoom_range:** Range for random zoom. [lower, upper] = [1-zoom_range, 1+zoom_range]`.\n",
    "\n",
    "\n",
    "After this, we are going to connect the '**train_datagen**' object to the training set, and to do this, we are going to import the training set, which can be done as given below. Here training_set is the name of the training set that we are importing in the notebook, and then we indeed take our train_datagen object so as to call the method of **ImageDataGenerator class**. The method that we will call is the **flow_from_directory** that will help to connect the image augmentation tool to the image of the training set. we will pass the following parameter;\n",
    "\n",
    "\n",
    "- The first parameter is the **path** leading to the training set.\n",
    "\n",
    "\n",
    "- The next parameter is the **target_size**, which is the final size of the images when they will be fed into the convolutional neural network.\n",
    "\n",
    "\n",
    "- The third one is the **batch_size**, which relates to the size of the batches, i.e., the total number of images we want to have in each batch. We have chosen 32, which is the classic default value.\n",
    "\n",
    "\n",
    "- Lastly, we will classify the **class mode** to be either binary or categorical. Since we are looking for a binary outcome, so will choose binary class mode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d790a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('dataset/training_set', target_size = (64, 64),\n",
    "                                                         batch_size = 32, class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66664aa7",
   "metadata": {},
   "source": [
    "From the above, we have got the output from the above image that indeed we imported and preprocessed with the data augmentation; 8048 images belonging to 2 classes.\n",
    "\n",
    "Now we are checking the those 2 classes by using 'class_indices' attribute, i.e., dogs and cats. The 'class_indices' will return the dictionary that contains the mapping from class names to class indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5880c112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642133a",
   "metadata": {},
   "source": [
    "<b> Preprocessing the Test set\n",
    "    \n",
    "After we are done with preprocessing the training set, we will further move on to preprocessing the test set. We will again take the ImageDataGenerator object to apply transformations to the test images, but here we will not apply the same transformations as we did in the previous step. However, we need to rescale their pixels the same as before because the future predict method of CNN will have to be applied to the same scaling as the one that was applied to the training set.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f618e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308f6c6e",
   "metadata": {},
   "source": [
    "After this, we are going to connect the '**test_datagen**' object to the **testing set**, and to do this, we are going to import the **testing set**, which can be done as given below. Here **test_set** is the name of the test set that we are importing in the notebook, and then we indeed take our **test_datagen**, which will only apply if it is going to the pixels of the test set images. Then we call the same '**flow_from_directory**' function to access the test set from the directory. Then we will need to have the same **target_size, batch_size, and class_mode** as used in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "882c8164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "testing_set = test_datagen.flow_from_directory('dataset/test_set', target_size = (64, 64),  \n",
    "                                                batch_size = 32, class_mode = 'binary')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d48ee16",
   "metadata": {},
   "source": [
    "We can see from the above image, which we got after running Preprocessing the Test Set cell, that 2000 images belong to 2 classes. Instead of applying image augmentation, we have only applied feature scaling.\n",
    "\n",
    "Now we are checking the those 2 classes by using 'class_indices' attribute, i.e., dogs and cats. The 'class_indices' will return the dictionary that contains the mapping from class names to class indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e565ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3863318b",
   "metadata": {},
   "source": [
    "### Modelling / Building the Convolution Neural Network (CNN)\n",
    "Hare we are going to build the convolutional neural network and, more specifically, the whole architecture of the artificial neural network. So, it is actually going to start the same as with our artificial neural network because the convolutional neural network is still a sequence of layers.\n",
    "\n",
    "\n",
    "Therefore, we are going to initialize our CNN with the same class, which is the sequential class.\n",
    "\n",
    "<b> Initializing the CNN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc53c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Sequential clas from keras.models library.\n",
    "from keras.models import Sequential \n",
    "\n",
    "# initialize the CNN - Sequential class as 'classifier'\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b840c2c",
   "metadata": {},
   "source": [
    "After this, we will step by step use the add method to add different layers, whether they are convolutional layers or fully connected layers, and in the end, the output layer. So, we are now going to successfully use the add method starting with the step1: convolution.\n",
    "\n",
    "<b> Step1: Convolution\n",
    "    \n",
    "Here we will first take the 'classifier' object or the convolutional neural network from which we will call the add method to add our very first convolutional layer, which will further be an object of a certain class, i.e., **Conv2D** class. And this class, just like the dense class that allows us to build a fully connected layer, belongs to the same module, which is the layer module from the Keras library.\n",
    "\n",
    "    \n",
    "Inside the class, we are going to pass four important parameters, which are as follows:\n",
    "\n",
    "- The first parameter is the **filters**, which is the number of feature detectors that we want to apply to images for feature detection.\n",
    "    \n",
    "    \n",
    "- The **kernel_size** is exactly the size of the feature detector, i.e., the number of rows, which is also the number of columns.\n",
    "    \n",
    "    \n",
    "- The third one is the **activation** but here we are not going to keep the default value for the activation parameter corresponding to the activation function, because indeed as long as we don't reach the output layer, we rather want to get a rectifier activation function. That is why we will choose the ReLU parameter name once again as it corresponds to the rectifier activation function.\n",
    "    \n",
    "    \n",
    "- Lastly, the **input_shape** parameter because it is necessary to specify the input shape of inputs. Since we are working with the colored images, so the input_shape will be [64, 64, 3].\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae0468ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Conv2D clas from keras.layers library.\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "# Convolution operation and relu\n",
    "classifier.add(Conv2D(input_shape=[64,64,3], filters=32, kernel_size=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e02dc0",
   "metadata": {},
   "source": [
    "<b> Step2: Pooling\n",
    "\n",
    "Next, we will move on to applying pooling, and more specifically, if we talk about, we are going to apply the max pooling, and for that, we will again take cnn object from which we are going to call our new method. Since we are adding the pooling layer to our convolutional layer, so we will again call the add method, and inside it, we will create an object of a max-pooling layer or an instance of a certain class, which is called **MaxPooling2D** class. Inside the class, we will pass **pool_size** and **strides** parameters. \n",
    "\n",
    "\n",
    "- pool_size: integer or tuple of 2 integers, window size over which to take the maximum. (2, 2) will take the max value over a 2x2 pooling window. If only one integer is specified, the same window length will be used for both dimensions.\n",
    "\n",
    "\n",
    "- strides: Integer, tuple of 2 integers, or None. Strides values. Specifies how far the pooling window moves for each pooling step. If None, it will default to pool_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e03dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing MaxPooling2D clas from keras.layers library.\n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "# applying the pooling classifier\n",
    "classifier.add(MaxPooling2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572622a9",
   "metadata": {},
   "source": [
    "<b> Step3: Flattening\n",
    "    \n",
    "In the third step, we will undergo flattening the result of these convolutions and pooling into a one-dimensional vector, which will become the input of a fully connected layer neural network. We will start with again taking our '**classifier**' object from which we will call the **add** method because the way we are going to create that flattening layer is once again by creating an instance of the **Flatten** class, such that Keras will automatically understand that this is the result of all these convolutions and pooling, which will be flattened into the one-dimensional vector.\n",
    "\n",
    "    \n",
    "So, we just need to specify that we want to apply flattening and to do this we will have to call once again the layers module by the Keras library from which we are actually going to call the flatten class, and we don't need to pass any kind of parameter inside it.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ee4876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Flatten clas from keras.layers library.\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# flattening the result\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619898ea",
   "metadata": {},
   "source": [
    "<b> Step4: Full Connection\n",
    "    \n",
    "In step 4, we are exactly in the same situation as before building a fully connected neural network. So, we will be adding a new fully-connected layer to that flatten layer, which is nothing but a one-dimensional vector that will become the input of a fully connected neural network. And for this, we will again start by taking a '**classifier**' neural network from which we are going to call the **add** method because now we are about to add a new layer, which is a fully connected layer that belongs to **keras.layers.** But this time, we will take a **Dense** class followed by passing **units**, which is the number of hidden neurons we want to have into this fully connected layer and **activation function** parameter.   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08e5cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Dense clas from keras.layers library.\n",
    "from keras.layers import Dense\n",
    "\n",
    "# hidden layer with 128 neurons\n",
    "classifier.add(Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb6d12",
   "metadata": {},
   "source": [
    "<b> Step5: Output Layer\n",
    "    \n",
    "Here we need to add the final output layer, which will be fully connected to the previous hidden layer. Therefore, we will call the **Dense** class once again in the same way as we did in the previous step but will change the value of the input parameters because the numbers of units in the output layer are definitely not 128. Since we are doing binary classification, it will actually be one neuron to encode that **binary class** into a 'cat' or 'dog'. And for the activation layer, it is recommended to have a **sigmoid** activation function. Since this is a binary classification problem, so we are using **sigmoid** activation function    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1b36564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output layer with 1 neuron\n",
    "classifier.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f6331f",
   "metadata": {},
   "source": [
    "### Training the CNN\n",
    "<b> Training the CNN model with train data & Testing the model with test data \n",
    "    \n",
    "Now we are going to train our CNN over 25 **epochs**, and at each epoch, we will actually see how our model is performing on our test set images. This is a different kind of training as we did before because we always used to separate the training and evaluation, but here this will happen at the same time as we are making some specific application, i.e., computer vision.  \n",
    "    \n",
    "<b> Compiling the CNN\n",
    "    \n",
    "Now we are going to compile the CNN, which means that we are going to connect it to an optimizer, a loss function, and some metrics. As we are doing once again a binary classification, so we are going to compile our CNN exactly the same way as we complied our ANN model because indeed, we are going to choose once again **adam** optimizer to perform **stochastic gradient descent** to update the weights in order to reduce the loss error between the predictions and target. Then we will choose the same loss, i.e., the **binary_crossentrophy** loss because we are doing exactly the same task binary classification. And then same for the metrics, we will choose **accuracy** metrics because it is the most relevant way to measure the performance of the classification model, which is exactly our case of CNN.\n",
    "\n",
    "So, we will take our 'classifier' object from which we will be calling the compile method that will take as input the optimizer, loss function, and the metrics.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "020588b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065acefd",
   "metadata": {},
   "source": [
    "<b> Training the CNN on the Training set and evaluation on the Test set\n",
    "    \n",
    "After the compilation, we will train the CNN on the training set followed by evaluating at the same time on the test set, which will not be exactly the same as before but will be somewhat similar. Basically, the first two steps are always the same, i.e., in the first step, we will take 'classifier' followed by taking the fit method in the second step that will train the 'classifier' on the training set. Inside it, we will pass the following parameters:\n",
    "\n",
    "- The first parameter is the set, which is off course the dataset (**training set**) on which we are going to train our model, and the name for that parameter is simply x.\n",
    "    \n",
    "    \n",
    "- The second parameter is the difference with what we did before. So, it has to do, of course with the fact that we are not only training the CNN on the training set but also evaluating it at the same time on the test set. And that is what exactly our second parameter corresponds to, so we will be specifying here the **validation data (testing set)**, which is the set on which we want to evaluate our CNN.\n",
    "    \n",
    "    \n",
    "Lastly, the **epochs** parameter, which is the number of epochs. Here we are choosing 25 epochs to converge the accuracy not only on the training set but also on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6fd78db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "252/252 [==============================] - 153s 600ms/step - loss: 0.6805 - accuracy: 0.6221 - val_loss: 0.5811 - val_accuracy: 0.6935\n",
      "Epoch 2/25\n",
      "252/252 [==============================] - 113s 448ms/step - loss: 0.5815 - accuracy: 0.6873 - val_loss: 0.5581 - val_accuracy: 0.7115\n",
      "Epoch 3/25\n",
      "252/252 [==============================] - 105s 418ms/step - loss: 0.5458 - accuracy: 0.7184 - val_loss: 0.5362 - val_accuracy: 0.7205\n",
      "Epoch 4/25\n",
      "252/252 [==============================] - 129s 512ms/step - loss: 0.5282 - accuracy: 0.7299 - val_loss: 0.5533 - val_accuracy: 0.7310\n",
      "Epoch 5/25\n",
      "252/252 [==============================] - 95s 374ms/step - loss: 0.4935 - accuracy: 0.7565 - val_loss: 0.5395 - val_accuracy: 0.7305\n",
      "Epoch 6/25\n",
      "252/252 [==============================] - 96s 379ms/step - loss: 0.4780 - accuracy: 0.7681 - val_loss: 0.7074 - val_accuracy: 0.6575\n",
      "Epoch 7/25\n",
      "252/252 [==============================] - 88s 348ms/step - loss: 0.4601 - accuracy: 0.7824 - val_loss: 0.5420 - val_accuracy: 0.7380\n",
      "Epoch 8/25\n",
      "252/252 [==============================] - 88s 349ms/step - loss: 0.4378 - accuracy: 0.7950 - val_loss: 0.5410 - val_accuracy: 0.7320\n",
      "Epoch 9/25\n",
      "252/252 [==============================] - 80s 317ms/step - loss: 0.4062 - accuracy: 0.8159 - val_loss: 0.6336 - val_accuracy: 0.7315\n",
      "Epoch 10/25\n",
      "252/252 [==============================] - 96s 382ms/step - loss: 0.4019 - accuracy: 0.8155 - val_loss: 0.5383 - val_accuracy: 0.7520\n",
      "Epoch 11/25\n",
      "252/252 [==============================] - 102s 405ms/step - loss: 0.3689 - accuracy: 0.8324 - val_loss: 0.5685 - val_accuracy: 0.7595\n",
      "Epoch 12/25\n",
      "252/252 [==============================] - 87s 344ms/step - loss: 0.3509 - accuracy: 0.8438 - val_loss: 0.5946 - val_accuracy: 0.7535\n",
      "Epoch 13/25\n",
      "252/252 [==============================] - 84s 334ms/step - loss: 0.3237 - accuracy: 0.8595 - val_loss: 0.6811 - val_accuracy: 0.7425\n",
      "Epoch 14/25\n",
      "252/252 [==============================] - 100s 398ms/step - loss: 0.3093 - accuracy: 0.8688 - val_loss: 0.6595 - val_accuracy: 0.7525\n",
      "Epoch 15/25\n",
      "252/252 [==============================] - 117s 464ms/step - loss: 0.2854 - accuracy: 0.8801 - val_loss: 0.6634 - val_accuracy: 0.7490\n",
      "Epoch 16/25\n",
      "252/252 [==============================] - 122s 484ms/step - loss: 0.2622 - accuracy: 0.8905 - val_loss: 0.6511 - val_accuracy: 0.7485\n",
      "Epoch 17/25\n",
      "252/252 [==============================] - 104s 410ms/step - loss: 0.2290 - accuracy: 0.9046 - val_loss: 0.7333 - val_accuracy: 0.7535\n",
      "Epoch 18/25\n",
      "252/252 [==============================] - 213s 847ms/step - loss: 0.2157 - accuracy: 0.9155 - val_loss: 0.7168 - val_accuracy: 0.7560\n",
      "Epoch 19/25\n",
      "252/252 [==============================] - 150s 595ms/step - loss: 0.2074 - accuracy: 0.9174 - val_loss: 0.7828 - val_accuracy: 0.7440\n",
      "Epoch 20/25\n",
      "252/252 [==============================] - 85s 335ms/step - loss: 0.1837 - accuracy: 0.9297 - val_loss: 0.8008 - val_accuracy: 0.7460\n",
      "Epoch 21/25\n",
      "252/252 [==============================] - 101s 400ms/step - loss: 0.1673 - accuracy: 0.9376 - val_loss: 0.8251 - val_accuracy: 0.7305\n",
      "Epoch 22/25\n",
      "252/252 [==============================] - 89s 354ms/step - loss: 0.1590 - accuracy: 0.9397 - val_loss: 0.9089 - val_accuracy: 0.7335\n",
      "Epoch 23/25\n",
      "252/252 [==============================] - 98s 389ms/step - loss: 0.1488 - accuracy: 0.9422 - val_loss: 0.8407 - val_accuracy: 0.7550\n",
      "Epoch 24/25\n",
      "252/252 [==============================] - 85s 338ms/step - loss: 0.1401 - accuracy: 0.9468 - val_loss: 0.8231 - val_accuracy: 0.7585\n",
      "Epoch 25/25\n",
      "252/252 [==============================] - 101s 400ms/step - loss: 0.1227 - accuracy: 0.9529 - val_loss: 0.9830 - val_accuracy: 0.7450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d2c0751a90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x = training_set, validation_data = testing_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e29f4d",
   "metadata": {},
   "source": [
    "<b> From the image given above, we would have ended up with an accuracy of around 95% on the training set, which clearly indicates overfitting and lower accuracy here on the test set around 74%. \n",
    "\n",
    "<b> So now we are going to rebuild the model with 'Dropout' function with '0.1'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d093f9",
   "metadata": {},
   "source": [
    "### Rebuilding the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4711344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "252/252 [==============================] - 144s 559ms/step - loss: 1.3387 - accuracy: 0.5859 - val_loss: 0.6409 - val_accuracy: 0.6195\n",
      "Epoch 2/25\n",
      "252/252 [==============================] - 104s 412ms/step - loss: 1.2841 - accuracy: 0.6553 - val_loss: 0.5663 - val_accuracy: 0.7135\n",
      "Epoch 3/25\n",
      "252/252 [==============================] - 101s 401ms/step - loss: 1.2929 - accuracy: 0.6644 - val_loss: 0.5979 - val_accuracy: 0.7040\n",
      "Epoch 4/25\n",
      "252/252 [==============================] - 104s 415ms/step - loss: 1.2669 - accuracy: 0.6827 - val_loss: 0.5957 - val_accuracy: 0.6730\n",
      "Epoch 5/25\n",
      "252/252 [==============================] - 87s 347ms/step - loss: 1.2916 - accuracy: 0.6940 - val_loss: 0.5617 - val_accuracy: 0.7090\n",
      "Epoch 6/25\n",
      "252/252 [==============================] - 88s 349ms/step - loss: 1.1746 - accuracy: 0.7107 - val_loss: 0.5335 - val_accuracy: 0.7440\n",
      "Epoch 7/25\n",
      "252/252 [==============================] - 89s 354ms/step - loss: 1.2206 - accuracy: 0.7249 - val_loss: 0.5414 - val_accuracy: 0.7400\n",
      "Epoch 8/25\n",
      "252/252 [==============================] - 88s 349ms/step - loss: 1.2636 - accuracy: 0.7157 - val_loss: 0.5824 - val_accuracy: 0.7050\n",
      "Epoch 9/25\n",
      "252/252 [==============================] - 90s 359ms/step - loss: 1.2403 - accuracy: 0.7174 - val_loss: 0.5630 - val_accuracy: 0.7270\n",
      "Epoch 10/25\n",
      "252/252 [==============================] - 116s 460ms/step - loss: 1.2123 - accuracy: 0.7399 - val_loss: 0.5225 - val_accuracy: 0.7535\n",
      "Epoch 11/25\n",
      "252/252 [==============================] - 129s 513ms/step - loss: 1.2107 - accuracy: 0.7578 - val_loss: 0.5225 - val_accuracy: 0.7445\n",
      "Epoch 12/25\n",
      "252/252 [==============================] - 121s 479ms/step - loss: 1.1763 - accuracy: 0.7587 - val_loss: 0.5236 - val_accuracy: 0.7535\n",
      "Epoch 13/25\n",
      "252/252 [==============================] - 107s 423ms/step - loss: 1.1803 - accuracy: 0.7639 - val_loss: 0.5164 - val_accuracy: 0.7540\n",
      "Epoch 14/25\n",
      "252/252 [==============================] - 92s 365ms/step - loss: 1.3085 - accuracy: 0.6918 - val_loss: 0.5619 - val_accuracy: 0.7230\n",
      "Epoch 15/25\n",
      "252/252 [==============================] - 87s 346ms/step - loss: 1.2072 - accuracy: 0.7539 - val_loss: 0.5257 - val_accuracy: 0.7515\n",
      "Epoch 16/25\n",
      "252/252 [==============================] - 86s 342ms/step - loss: 1.2755 - accuracy: 0.7396 - val_loss: 0.5532 - val_accuracy: 0.7420\n",
      "Epoch 17/25\n",
      "252/252 [==============================] - 85s 338ms/step - loss: 1.1598 - accuracy: 0.7650 - val_loss: 0.5423 - val_accuracy: 0.7490\n",
      "Epoch 18/25\n",
      "252/252 [==============================] - 87s 344ms/step - loss: 1.0747 - accuracy: 0.7853 - val_loss: 0.5434 - val_accuracy: 0.7545\n",
      "Epoch 19/25\n",
      "252/252 [==============================] - 86s 339ms/step - loss: 1.1258 - accuracy: 0.7822 - val_loss: 0.5262 - val_accuracy: 0.7535\n",
      "Epoch 20/25\n",
      "252/252 [==============================] - 91s 360ms/step - loss: 1.1739 - accuracy: 0.7592 - val_loss: 0.5465 - val_accuracy: 0.7365\n",
      "Epoch 21/25\n",
      "252/252 [==============================] - 90s 356ms/step - loss: 1.1006 - accuracy: 0.7939 - val_loss: 0.5445 - val_accuracy: 0.7465\n",
      "Epoch 22/25\n",
      "252/252 [==============================] - 86s 339ms/step - loss: 1.0140 - accuracy: 0.8172 - val_loss: 0.5497 - val_accuracy: 0.7500\n",
      "Epoch 23/25\n",
      "252/252 [==============================] - 86s 342ms/step - loss: 1.0299 - accuracy: 0.8205 - val_loss: 0.5739 - val_accuracy: 0.7395\n",
      "Epoch 24/25\n",
      "252/252 [==============================] - 114s 451ms/step - loss: 1.0996 - accuracy: 0.8088 - val_loss: 0.5428 - val_accuracy: 0.7550\n",
      "Epoch 25/25\n",
      "252/252 [==============================] - 121s 480ms/step - loss: 1.0807 - accuracy: 0.7954 - val_loss: 0.5494 - val_accuracy: 0.7440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d2c4fb4040>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing Sequential clas from keras.models library.\n",
    "from keras.models import Sequential \n",
    "\n",
    "# initialize the CNN - Sequential class as 'classifier'\n",
    "model = Sequential()\n",
    "\n",
    "# importing Conv2D clas from keras.layers library.\n",
    "from keras.layers import Conv2D, Dropout\n",
    "\n",
    "# Convolution operation and relu\n",
    "model.add(Conv2D(input_shape=[64,64,3], filters=32, kernel_size=3, activation='relu'))\n",
    "\n",
    "# importing MaxPooling2D clas from keras.layers library.\n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "# applying the pooling classifier\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(Dropout(0.1))                              # Dropout with 10% \n",
    "\n",
    "# importing Flatten clas from keras.layers library.                     \n",
    "from keras.layers import Flatten\n",
    "\n",
    "# flattening the result\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.1))                              # Dropout with 10% \n",
    "\n",
    "# importing Dense clas from keras.layers library.\n",
    "from keras.layers import Dense\n",
    "\n",
    "# hidden layer with 128 neurons\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.1))                              # Dropout with 10% \n",
    "\n",
    "# output layer with 1 neuron\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.add(Dropout(0.1))                              # Dropout with 10% \n",
    "\n",
    "# Compiling the CNN\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
    "\n",
    "# Training the CNN on the Training set and evaluation on the Test set\n",
    "model.fit(x = training_set, validation_data = testing_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc16158",
   "metadata": {},
   "source": [
    "<b> From the image given above, it can be seen that we ended with 79% of final accuracy on the training set and final accuracy of 74% on the test set. Based on this, we conclude that we have got good model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91805595",
   "metadata": {},
   "source": [
    "### Evaluation:\n",
    "<b> Making a single prediction\n",
    "    \n",
    "Here we will make a single prediction, based on the rebuild model. Now we are performing evaluation on the two separate images of this single prediction folder for which our model will have to recognize for both the dog and cat, respectively.\n",
    "\n",
    "    \n",
    "To display the image, '**PIL**' library is using an '**Image**' class within it. The image module inside pillow package contains some important inbuilt functions like, load images or create new images, etc.\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "045844d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584159a1",
   "metadata": {},
   "source": [
    "To load the image, we simply import the '**Image**' module from the 'PIL' and call the **Image.open()**, passing the image filename. Which returns a value of the Image object data type. Any modification we make to the image object can be saved to an image file with the save() method. The image object we received using Image.open(), later can be used to **resize** image manipulation method calls on this Image object.\n",
    "\n",
    "\n",
    "But to make our first **test _set** image accepted by the predict method, we need to convert the format of an image into an array because the predict method expects its input to be a 2D array. And we will do this with the help of another function of the image preprocessing module, i.e., **np.array()** function, which indeed converts PIL image instance into a NumPy array that is exactly the format of array expected by the predict method. We are going to use the np.array(), and inside, it will take the test_size image in PIL format that we are looking forward to convert it into the NumPy array format.\n",
    "\n",
    "\n",
    "Next, we will add an extra dimension, which will correspond to the batch that will contain that image into the batch, and it can be simply done by updating our test image by adding extra dimensions corresponding to batch. And the way to do it is with NumPy as the NumPy arrays can be easily manipulated, so we will first call the NumPy from which we will call this function that allows exactly to add a fake dimension, or we can say a dimension corresponding to the batch, which is called **expand_dims** function inside of which we will input the image to which we want to add this extra dimension corresponding to the batch followed by adding an extra argument, which is an axis that we have to set equal to **zero**. That is why the dimension of a batch that we want to add to our image will be the first dimension.\n",
    "\n",
    "\n",
    "After this, we are going to call the predict method on that '**test_image**' and saving the same result as 'result' object.\n",
    "\n",
    "After this, inside the batch, we are going to get access to the first element of the batch that corresponds to the prediction of that same cat_or_dog_1 image. As we are dealing with a single image, so a single prediction is needed, and to get that, we will need to get inside the batch of index zero, the first and only prediction once again, which has a [0] index. So, that is how we get our prediction by first accessing the batch followed by accessing the single element of the batch, and if that prediction equals to one, then we already know that it corresponds to the dog, then we will create a new variable which we will call as prediction and will set that prediction variable equals to the dog. Likewise, in the else condition, if the result prediction equals to 1, then the prediction will be a cat. Now we will wrap it up by simply printing the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4af43937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 675ms/step\n",
      "Dog\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "test_image = Image.open('dataset\\single_prediction\\cat_or_dog_1.jpg')\n",
    "\n",
    "# Data Preprocessing\n",
    "test_image = test_image.resize((64, 64))\n",
    "test_image = np.array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "# Prediction\n",
    "result = model.predict(test_image)\n",
    "\n",
    "# Evaluation\n",
    "if result[0][0]==1:         # If result=1 then print Dog else Cat\n",
    "    print('Dog')\n",
    "else:\n",
    "    print('Cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca180fcf",
   "metadata": {},
   "source": [
    "<b> We can see our Convolution Neural Network predicted that there is a dog inside the image. So, it can be concluded that our first test is passed successfully.\n",
    "    \n",
    "    \n",
    "<b> Now we will check for the other image which is of the cat, so for that we will need to deploy our model on this single image and check that indeed, our CNN returns a cat. To do this, we need to change the name here, i.e. cat_or_dog_2.jpg and then play this cell again by clicking on the Run button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa6813ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 60ms/step\n",
      "Cat\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "test_image = Image.open('dataset\\single_prediction\\cat_or_dog_2.jpg')\n",
    "\n",
    "# Data Preprocessing\n",
    "test_image = test_image.resize((64, 64))\n",
    "test_image = np.array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "# Prediction\n",
    "result = model.predict(test_image)\n",
    "\n",
    "# Evaluation\n",
    "if result[0][0]==1:         # If result=1 then print Dog else Cat\n",
    "    print('Dog')\n",
    "else:\n",
    "    print('Cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16f201",
   "metadata": {},
   "source": [
    "<b> So, it's clear now that our CNN model is successful in predicting cat in the output of the console. Hence our CNN got all the answers correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
