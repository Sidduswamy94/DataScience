{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ea4b0a",
   "metadata": {},
   "source": [
    "# <p style='text-align: center;'> Introduction to Dimension Reduction in Machine Learning </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c2f996",
   "metadata": {},
   "source": [
    "- Dimensionality reduction technique can be defined as, \"It is a way of converting the higher dimensions dataset into lesser dimensions dataset ensuring that it provides similar information\".\n",
    "\n",
    "\n",
    "- Generally, in data-set they will be a multiple features, out of which some of them are important and some of them are unimportant. So identifying and deleting (dropping) the unimportant features is known as **Dimension Reduction**. Before deleting (dropping) the unimportant features we need to prove statistically as these features are unimportant.\n",
    "\n",
    "\n",
    "- The number of input features, variables, or columns present in a given dataset is known as dimensionality, and the process to reduce these features is called dimensionality reduction.\n",
    "\n",
    "\n",
    "- A dataset contains a huge number of input features in various cases, which makes the predictive modeling task more complicated. Because it is very difficult to visualize or make predictions for the training dataset with a high number of features, for such cases, dimensionality reduction techniques are required to use.\n",
    "\n",
    "\n",
    "- These techniques are widely used in machine learning for obtaining a better fit predictive model while solving the classification and regression problems.\n",
    "\n",
    "\n",
    "- It is commonly used in the fields that deal with high-dimensional data, such as speech recognition, signal processing, bioinformatics, etc. It can also be used for data visualization, noise reduction, cluster analysis, etc.\n",
    "\n",
    "\n",
    "- Handling the high-dimensional data is very difficult in practice, commonly known as the **curse of dimensionality**. If the dimensionality of the input dataset increases, any machine learning algorithm and model becomes more complex. As the number of features increases, the number of samples also gets increased proportionally, and the chance of overfitting also increases. If the machine learning model is trained on high-dimensional data, it becomes overfitted and results in poor performance. Hence, it is often required to reduce the number of features, which can be done with dimensionality reduction.\n",
    "\n",
    "\n",
    "### Advantages (Benefits of applying) of dimensionality Reduction:\n",
    "- By reducing the dimensions of the features, the space required to store the dataset also gets reduced.\n",
    "\n",
    "\n",
    "- Less Computation training time is required for reduced dimensions of features.\n",
    "\n",
    "\n",
    "- Reduced dimensions of features of the dataset help in visualizing the data quickly.\n",
    "\n",
    "\n",
    "- It removes the redundant features (if present) by taking care of multicollinearity.\n",
    "\n",
    "\n",
    "### Disadvantages of dimensionality Reduction:\n",
    "- Some data may be lost due to dimensionality reduction.\n",
    "\n",
    "\n",
    "- In the PCA dimensionality reduction technique, sometimes the principal components required to consider are unknown.\n",
    "\n",
    "\n",
    "## Approaches of Dimension Reduction:\n",
    "<b> There are two ways to apply the dimension reduction technique, which are given below:\n",
    "\n",
    "   - Feature Selection.\n",
    "   - Feature Extraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
