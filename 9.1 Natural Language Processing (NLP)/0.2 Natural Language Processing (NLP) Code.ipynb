{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b6547e",
   "metadata": {},
   "source": [
    "## In this article, we are going to perform (apply) the coding on paragraph by using NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8a7f0",
   "metadata": {},
   "source": [
    "<b> Importing and downloading the nltk library and packages.\n",
    "\n",
    "<b> Install the NLTK library using pip command.\n",
    "\n",
    "If it is already installed or present in your notebook, it will prompt Requirement already Satisfied otherwise it will start downloading and start installing the NLTK library in your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b3ce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\administrator\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "# Install the NLTK library using pip command.\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd15939c",
   "metadata": {},
   "source": [
    "<b> In my case it's already installed, hence it's shown as \"Requirement already Satisfied\".\n",
    "    \n",
    "<b> Import the NLTK library:\n",
    "    \n",
    "By using the \"**import nltk**\" command we can import the nltk library for your further operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0b26b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk library\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903846b",
   "metadata": {},
   "source": [
    "<b> Installing All from NLTK library\n",
    "\n",
    "If we want to download all packages from the NLTk library then by using the \"**nltk.download()**\" command we can download the packages which will unzipp all the packages from NLTK Corpus like for e.g. Stemmer, lemmatizer and many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c46564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download nltk all the packages\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277ad9d",
   "metadata": {},
   "source": [
    "<b> Writing the paragraph, which one we are going to perform NLP coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ca0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"He is a good boy. She is a good girl. boy & girl are good.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb5e62",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "<b> Tokenization means splitting text into meaningful unit words. There are sentence tokenizers as well as word tokenizers.\n",
    "    \n",
    "<b> Sentence Tokenization\n",
    "    \n",
    "Sentence tokenizer splits a paragraph into meaningful sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91677e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He is a good boy.', 'She is a good girl.', 'boy & girl are good.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# print the result\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64185338",
   "metadata": {},
   "source": [
    "<b> Word  Tokenization\n",
    "    \n",
    "word tokenizer splits a sentence into unit meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e7e55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'boy',\n",
       " '.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'girl',\n",
       " '.',\n",
       " 'boy',\n",
       " '&',\n",
       " 'girl',\n",
       " 'are',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "\n",
    "# print the result\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3dfb84",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "<b> Punctuation Removal:\n",
    "    \n",
    "- Removing punctuation is a crucial step since punctuation doesn’t add any extra information or value to our data. Hence, removing punctuation reduces the data size; therefore, it improves computational efficiency.\n",
    "    \n",
    "    \n",
    "- The Punctuations are: ('!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~') ---> Excluded paranthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40a1560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He is a good boy ', 'She is a good girl ', 'boy   girl are good ']\n"
     ]
    }
   ],
   "source": [
    "# import the regular expression \n",
    "import re\n",
    "\n",
    "# initilise the \"corpus\" empty list\n",
    "corpus = []\n",
    "\n",
    "# Using for loop for iteration each sentences.\n",
    "for i in range(len(sentences)):\n",
    "    # remove the punctuation and same store as \"rp\" object. \n",
    "    rp = re.sub('[^a-zA-Z]',\" \", sentences[i])\n",
    "    # append the result to corpus list\n",
    "    corpus.append(rp)\n",
    "    \n",
    "    \n",
    "# print the corpus\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd421b95",
   "metadata": {},
   "source": [
    "<b> Stop Words Removal:\n",
    "    \n",
    "- Words that frequently occur in sentences and carry no significant meaning in sentences. These are not important for prediction, so we remove stopwords to reduce data size and prevent overfitting. Note: Before filtering stopwords, make sure you lowercase the data since our stopwords are lowercase.\n",
    "    \n",
    "    \n",
    "- Using the NLTK library, we can filter out our Stopwords from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3675d8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good boy', 'good girl', 'boy girl good']\n"
     ]
    }
   ],
   "source": [
    "# import stopwords class from nltk.corpus library.\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")\n",
    "\n",
    "# initilise the \"corpus\" empty list\n",
    "corpus = []\n",
    "\n",
    "# Using for loop for iteration each records.\n",
    "for i in range(len(sentences)):\n",
    "    # remove the punctuation and same store as \"rp\" object. \n",
    "    rp = re.sub('[^a-zA-Z]',\" \", sentences[i])\n",
    "    # lowering the \"rp\" object and storing in the same object.\n",
    "    rp = rp.lower()\n",
    "    # split the words from sentences and same store in the same object.\n",
    "    rp = rp.split()\n",
    "    # removing the stopwords (by using list comprehension)\n",
    "    rp = [word for word in rp if not word in set(stopwords.words('english'))]\n",
    "    # join the words.\n",
    "    rp = \" \".join(rp)\n",
    "    # append the result to corpus list\n",
    "    corpus.append(rp)\n",
    "    \n",
    "# print the corpus\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98245433",
   "metadata": {},
   "source": [
    "<b> Stemming\n",
    "\n",
    "Stemming is converting words into their root word using some set of rules irrespective of meaning.\n",
    "\n",
    "Stemming may or may not returns meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac6c1d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import PoasterStemmer class from nltk.stem library\n",
    "from nltk.stem import PorterStemmer    \n",
    "\n",
    "# Initializa the PorterStemmer as \"ps\"\n",
    "ps = PorterStemmer()          \n",
    "\n",
    "# applying stemming on \"history\" word\n",
    "ps.stem(\"history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca5767",
   "metadata": {},
   "source": [
    "<b> Lemmatization\n",
    "    \n",
    "Lemmatization is converting words into their root word using vocabulary mapping. Lemmatization is done with the help of part of speech and its meaning; hence it doesn’t generate meaningless root words. But lemmatization is slower than stemming.  \n",
    "    \n",
    "Lemmatizatio must be returns meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d68537e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import WordNetLemmatizer class from nltk.stem library\n",
    "from nltk.stem import WordNetLemmatizer   \n",
    "\n",
    "# Initializa the WordNetLemmatizer as \"wnl\"\n",
    "wnl = WordNetLemmatizer()          \n",
    "\n",
    "# applying stemming on \"history\" word\n",
    "wnl.lemmatize(\"history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fbeac1",
   "metadata": {},
   "source": [
    "## Text Cleaning (Punctuation Removal + Stop Words Removal + Stemming/Lemmatization)\n",
    "\n",
    "<b> Here we are going to apply all these steps in the single page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "984ff1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good boy', 'good girl', 'boy girl good']\n"
     ]
    }
   ],
   "source": [
    "# initilise the \"corpus\" empty list\n",
    "corpus = []\n",
    "\n",
    "# Using for loop for iteration each records.\n",
    "for i in range(len(sentences)):\n",
    "    # remove the punctuation and same store as \"rp\" object. \n",
    "    rp = re.sub('[^a-zA-Z]',\" \", sentences[i])\n",
    "    # lowering the \"rp\" object and storing in the same object.\n",
    "    rp = rp.lower()\n",
    "    # split the words from sentences and same store in the same object.\n",
    "    rp = rp.split()\n",
    "    # converting words into their root word by stemming (by using list comprehension)\n",
    "    rp = [ps.stem(word) for word in rp if not word in (stopwords.words('english'))]\n",
    "    # join the words.\n",
    "    rp = \" \".join(rp)\n",
    "    # append the result to corpus list\n",
    "    corpus.append(rp)\n",
    "    \n",
    "# print the corpus\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daaf208",
   "metadata": {},
   "source": [
    "## Vectorizaton:\n",
    "Vectorization is the process of converting textual data into numerical vectors and is a process that is usually applied once the text is cleaned.\n",
    "\n",
    "<b> 1. CountVectorizer (bag of words):\n",
    "    \n",
    "- CountVectorizer is one of the simplest techniques that is used for converting text into vectors. It starts by tokenizing the document into a list of tokens (words). It selects the unique tokens from the token list and creates a vocabulary of words. Finally, a sparse matrix is created containing the frequency of words, where each row represents different sentences and each column represents unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "012fb9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the CountVectorizer() class from sklearn.feature_extraction.text library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# creating object cv of class CountVectorizer()\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus and same store as \"bow\" object.\n",
    "bow = cv.fit_transform(corpus).toarray()          # toarray returns an ndarray\n",
    "\n",
    "# print the result\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c1796",
   "metadata": {},
   "source": [
    "<b> 2. TF-IDF:\n",
    "    \n",
    "- TF-IDF or **Term Frequency–Inverse Document Frequency**, is a statistical measure that tells how relevant a word is to a document. It combines two metrics — term frequency and inverse document frequency — to produce a relevance score.\n",
    "\n",
    "\n",
    "The **Term Frequency** is the frequency of a word in a document. It is calculated by dividing the occurrence of a word inside a document by the total number of words in that document.\n",
    "\n",
    "\n",
    "The **Inverse Document Frequency** is a measure of how much information a word provides. Words like “the,” for example, occur very frequently but provide little context or value to a sentence. It is calculated by taking the inverse log of document frequency, that is the proportion of documents that contain a particular word.\n",
    "\n",
    "- TF-IDF scores range from 0 to 1. A score closer to 1 is higher the importance of a word to a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1fd7663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78980693, 0.        , 0.61335554],\n",
       "       [0.        , 0.78980693, 0.61335554],\n",
       "       [0.61980538, 0.61980538, 0.48133417]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the TfidfVectorizer() class from sklearn.feature_extraction.text library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# creating object tf of class TfidfVectorizer()\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus and same store as \"tfidf\" object.\n",
    "tfidf = tf.fit_transform(corpus).toarray()          # toarray returns an ndarray\n",
    "\n",
    "# print the result\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62495b00",
   "metadata": {},
   "source": [
    "<b> 3. N-grams\n",
    "\n",
    "- N-gram can be defined as the contiguous sequence of n items from a given sample of text or speech. The items can be letters, words, or base pairs according to the application. The N-grams typically are collected from a text or speech corpus (A long text dataset).\n",
    "\n",
    "\n",
    "- N-grams means number of words which we will combine together.\n",
    "    \n",
    "    \n",
    "- In order to implement n-grams, ngrams function present in nltk is used which will perform all the n-gram operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d629f2",
   "metadata": {},
   "source": [
    "<b> Writing the sentence, which one we are going to perform N-gram coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d960ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"This is a sentence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3eb8eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This', 'is')\n",
      "('is', 'a')\n",
      "('a', 'sentence')\n"
     ]
    }
   ],
   "source": [
    "# import the ngrams from nltk library\n",
    "from nltk import ngrams\n",
    "\n",
    "# creating object \"n-grams\" of class ngrams()\n",
    "n_grams = ngrams(sentence2.split(), 2)\n",
    "\n",
    "# Using for loop for iteration each words.\n",
    "for grams in n_grams:\n",
    "    print(grams)         # print the words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127e7fb",
   "metadata": {},
   "source": [
    "In the above first we have imported ngrams class from nltk library. After importing the class, we have created the Classifier object of the class. The Parameter of this class are as :\n",
    "\n",
    "- param sequence: the source data to be converted into ngrams.\n",
    "\n",
    "\n",
    "- type n: type of ngram, i.e. unigram, bigram, trigram and n-grams.\n",
    "\n",
    "And then we have used for loop for iteration of each words and finally printed the result as required. In the above we have passed type_n=2, i.e. bigram. If we want a unigram or n-gram we need to change the type_n value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b3b968",
   "metadata": {},
   "source": [
    "## Part-of-Speech (POS) Tagging\n",
    "- Part-of-Speech (POS) tagging a process of assigning one of the parts of speech to the given word.\n",
    "\n",
    "\n",
    "- if we talk about Part-of-Speech (POS) tagging, it may be defined as the process of converting a sentence in the form of a list of words, into a list of tuples. Here, the tuples are in the form of (word, tag).\n",
    "\n",
    "\n",
    "**Example:**\n",
    "Let us understand it with a Python experiment −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5dba621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('school', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# import the nltk library.\n",
    "import nltk\n",
    "\n",
    "# import the word_tokenize class from nltk library.\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Create a sentence.\n",
    "sentence3 = \"I am going to school\"\n",
    "\n",
    "# Calculate the Part-of-Speech (POS) Tagging on above sentence by using pos_tag()\n",
    "# And preint the result.\n",
    "print (nltk.pos_tag(word_tokenize(sentence3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7bfcc",
   "metadata": {},
   "source": [
    "<b> In the above, we have got the part of speech of each words from the sentence and regarding \"PRP\", \"VBP\", \"VBG\" we have discussed in the 0.1 Natural Language Processing (NLP) Notes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
